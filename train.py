import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from dataset import SuperResolutionDataset
from model import Generator, Discriminator
import torch.nn.functional as F
import os

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
epochs = 10
batch_size = 24
lr_g = 3e-4  # Generatorã®å­¦ç¿’ç‡
lr_d = 1e-7  # Discriminatorã®å­¦ç¿’ç‡

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
dataset = SuperResolutionDataset(low_res_dir="data/low", high_res_dir="data/high")
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
generator = Generator().cuda()
discriminator = Discriminator(img_size=512).cuda()

# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®è¨­å®š
optim_g = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.9, 0.999))
optim_d = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.9, 0.999))

# æå¤±é–¢æ•°
criterion = torch.nn.MSELoss()  # Adversarial Loss

# é€”ä¸­ã‹ã‚‰å†é–‹ã™ã‚‹ãŸã‚ã®å¤‰æ•°
start_epoch = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ã‹ã‚‰é–‹å§‹

# ã‚‚ã—ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚Œã°ã€èª­ã¿è¾¼ã‚€
checkpoint_path = "checkpoint.pth"
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    generator.load_state_dict(checkpoint["generator_state_dict"])
    discriminator.load_state_dict(checkpoint["discriminator_state_dict"])
    optim_g.load_state_dict(checkpoint["optimizer_g_state_dict"])
    optim_d.load_state_dict(checkpoint["optimizer_d_state_dict"])
    start_epoch = checkpoint["epoch"] + 1  # æ¬¡ã®ã‚¨ãƒãƒƒã‚¯ã‹ã‚‰é–‹å§‹
    print(f"âœ… å­¦ç¿’ã‚’ {start_epoch} ã‚¨ãƒãƒƒã‚¯ç›®ã‹ã‚‰å†é–‹ã—ã¾ã™ã€‚")
else:
    print("ğŸš€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚‚å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ–°è¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
for epoch in range(start_epoch, epochs + 1):
    for batch_idx, (lr, hr) in enumerate(dataloader):
        # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«é€ã‚‹
        lr, hr = lr.cuda(), hr.cuda()

        # -----------------
        # Discriminatorã®è¨“ç·´
        # -----------------
        optim_d.zero_grad()

        # æœ¬ç‰©ã®ç”»åƒã¨ç”Ÿæˆã•ã‚ŒãŸç”»åƒ
        real_output = discriminator(hr)
        fake_hr = generator(lr)
        fake_output = discriminator(fake_hr.detach())  # å­¦ç¿’ã‚’ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã—ãªã„

        # æœ¬ç‰©ã¨å½ç‰©ã®åˆ¤å®šæå¤±
        real_loss = criterion(real_output, torch.ones_like(real_output))
        fake_loss = criterion(fake_output, torch.zeros_like(fake_output))
        d_loss = (real_loss + fake_loss) / 2
        d_loss.backward()
        optim_d.step()

        # -----------------
        # Generatorã®è¨“ç·´
        # -----------------
        optim_g.zero_grad()

        # Generatorã®å‡ºåŠ›ã‚’Discriminatorã«é€šã—ã¦ã€æå¤±ã‚’è¨ˆç®—
        fake_output = discriminator(fake_hr)
        g_loss = criterion(fake_output, torch.ones_like(fake_output))  # æœ¬ç‰©ã¨èªè­˜ã•ã›ã‚‹

        g_loss.backward()
        optim_g.step()

        if batch_idx % 100 == 0:
            print(f"Epoch [{epoch}/{epochs}], Step [{batch_idx}/{len(dataloader)}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}")
    
    # 5ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    if epoch % 5 == 0:
        torch.save(generator.state_dict(), f"generator_epoch_{epoch}.pth")
        torch.save(discriminator.state_dict(), f"discriminator_epoch_{epoch}.pth")
        torch.save({
            "epoch": epoch,
            "generator_state_dict": generator.state_dict(),
            "discriminator_state_dict": discriminator.state_dict(),
            "optimizer_g_state_dict": optim_g.state_dict(),
            "optimizer_d_state_dict": optim_d.state_dict(),
        }, "checkpoint.pth")
        print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆEpoch {epoch}ï¼‰")

torch.save(generator.state_dict(), f"generator_final.pth")
torch.save(discriminator.state_dict(), f"discriminator_final.pth")